{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: Carlos Arbones & Benet Ramió"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collisions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider that the preprocessing we carried out in the previous practice is correct for this project as well. Therefore, the only thing we need to modify is to select only the rows that belong to AMBULANCES, TAXIS, and FIRE TRUCKS, as stated in the project description. También seleccionamos solo las muestras del 2018 i eleliminamos las columnas que no seran necesarias para el estudio.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions = pd.read_csv('code/data/preprocessed-collisions.csv') \n",
    "collisions['CRASH_DATETIME'] = pd.to_datetime(collisions['CRASH_DATE'] + ' ' + collisions['CRASH_TIME'], format='%m/%d/%Y %H:%M')\n",
    "collisions = collisions.drop(columns=['CRASH_DATE', 'CRASH_TIME', 'CONTRIBUTING_FACTOR_VEHICLE2', 'VEHICLE_TYPE_CODE2'])\n",
    "collisions = collisions[collisions['CRASH_DATETIME'].dt.year == 2018] # Only 2018 data\n",
    "collisions = collisions[collisions['VEHICLE_TYPE_CODE1'].isin(['Taxi', 'Ambulance', 'Fire truck'])].reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4093, 15)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH                         1385\n",
       "ZIP_CODE                        1385\n",
       "LATITUDE                         294\n",
       "LONGITUDE                        294\n",
       "TOTAL_INJURED                      1\n",
       "TOTAL_KILLED                       1\n",
       "PEDESTRIANS_INJURED                0\n",
       "PEDESTRIANS_KILLED                 0\n",
       "CYCLIST_INJURED                    0\n",
       "CYCLIST_KILLED                     0\n",
       "MOTORIST_INJURED                   0\n",
       "MOTORIST_KILLED                    0\n",
       "CONTRIBUTING_FACTOR_VEHICLE1       0\n",
       "VEHICLE_TYPE_CODE1                 0\n",
       "CRASH_DATETIME                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 1385 missing values in the 'BOROUGH' and 'ZIP_CODE' columns and only 294 missing values in the coordinates, we will impute the values where we have LATITUDE and LONGITUDE but are missing 'BOROUGH' or 'ZIP_CODE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def get_location_info(latitude, longitude):\n",
    "    geolocator = Nominatim(user_agent=\"my_geocoder\") # initialize geolocator\n",
    "    location = geolocator.reverse((latitude, longitude), language=\"en\") # reverse geocoding\n",
    "    borough = location.raw['address']['suburb'].upper() if 'suburb' in location.raw['address'] else location.raw['address']['city'].upper()\n",
    "    zip_code = location.raw['address']['postcode'] if 'postcode' in location.raw['address'] else None\n",
    "\n",
    "    return borough, zip_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_borough_or_zip = (collisions['BOROUGH'].isna() | collisions['ZIP_CODE'].isna()) # filter for rows with missing borough or zip code\n",
    "has_lat_long = (collisions['LATITUDE'].notna() & collisions['LONGITUDE'].notna()) # filter for rows with latitude and longitude\n",
    "\n",
    "missing_locations = collisions[no_borough_or_zip & has_lat_long] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1247it [10:32,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# import tqdm\n",
    "# for row in tqdm.tqdm(missing_locations.itertuples()):\n",
    "#     borough, zip_code = get_location_info(row.LATITUDE, row.LONGITUDE)\n",
    "#     collisions.loc[row.Index, 'BOROUGH'] = borough\n",
    "#     collisions.loc[row.Index, 'ZIP_CODE'] = zip_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH\n",
       "MANHATTAN        2332\n",
       "BROOKLYN          597\n",
       "QUEENS            332\n",
       "BRONX             306\n",
       "QUEENS COUNTY     216\n",
       "THE BRONX         159\n",
       "STATEN ISLAND       9\n",
       "KINGS COUNTY        4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['BOROUGH'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We corrected some inconsistencies by changing neighborhood names from 'THE BRONX' to 'BRONX' and from 'QUEENS COUNTY' to 'QUEENS'. Additionally, we removed the rows that had 'KINGS COUNTY' as the neighborhood since it refers to a neighborhood in California. We assume that the data for those rows is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "collisions.loc[collisions['BOROUGH'] == 'THE BRONX', 'BOROUGH'] = 'BRONX' # standardize borough names\n",
    "collisions.loc[collisions['BOROUGH'] == 'QUEENS COUNTY', 'BOROUGH'] = 'QUEENS' # standardize borough names\n",
    "collisions = collisions[collisions['BOROUGH'] != 'KINGS COUNTY'].reset_index(drop=True) # remove rows with borough 'KINGS COUNTY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH\n",
       "MANHATTAN        2332\n",
       "BROOKLYN          597\n",
       "QUEENS            548\n",
       "BRONX             465\n",
       "STATEN ISLAND       9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions['BOROUGH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOROUGH                         0.033749\n",
       "ZIP_CODE                        0.033994\n",
       "LATITUDE                        0.071900\n",
       "LONGITUDE                       0.071900\n",
       "TOTAL_INJURED                   0.000245\n",
       "TOTAL_KILLED                    0.000245\n",
       "PEDESTRIANS_INJURED             0.000000\n",
       "PEDESTRIANS_KILLED              0.000000\n",
       "CYCLIST_INJURED                 0.000000\n",
       "CYCLIST_KILLED                  0.000000\n",
       "MOTORIST_INJURED                0.000000\n",
       "MOTORIST_KILLED                 0.000000\n",
       "CONTRIBUTING_FACTOR_VEHICLE1    0.000000\n",
       "VEHICLE_TYPE_CODE1              0.000000\n",
       "CRASH_DATETIME                  0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentatge of rows with missing values\n",
    "collisions.isna().sum() / collisions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the maximum percentage of missing values is in Latitude and Longitude, with 7% of the rows containing null values. To maintain data consistency, we have decided to delete the rows that contain null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3793, 15)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collisions = collisions.dropna().reset_index(drop=True) # drop rows with missing values\n",
    "collisions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collisions.to_csv('code/data/preprocessed-collisions-2.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the weather dataset, which contains only the data we are interested in, i.e., from June to September 2018. Therefore, there is no need to filter by year. Additionally, for our visualizations and to answer the questions, we will only use the 'icon' column, which contains the type of weather condition for each day. No missing values are observed in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>icon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-06-02</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-06-03</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-06-04</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-06-05</td>\n",
       "      <td>partly-cloudy-day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    datetime               icon\n",
       "0 2018-06-01               rain\n",
       "1 2018-06-02               rain\n",
       "2 2018-06-03               rain\n",
       "3 2018-06-04               rain\n",
       "4 2018-06-05  partly-cloudy-day"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.read_csv('code/data/weather2018.csv')\n",
    "weather = weather[['datetime', 'icon']] # keep only datetime and icon columns\n",
    "weather['datetime'] = pd.to_datetime(weather['datetime']) # convert datetime column to datetime type\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "icon\n",
       "rain                 75\n",
       "clear-day            23\n",
       "partly-cloudy-day    22\n",
       "cloudy                2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather['icon'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime    0\n",
       "icon        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
